#imports
%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import importlib
import sys
import os


import shared_utils


# --- User toggle ---
Cristina = True
Shar = not Cristina 

# --- System Configurations ---

if Cristina:
    print("[CONFIG] Using Cristina's local MacBook setup")
    sys.path.insert(0, "/Users/andradenebula/Documents/Research/Transient_Metrics/Multi_Transient_Metrics_Hub")
    os.environ["RUBIN_SIM_DATA_DIR"] = "/Users/andradenebula/rubin_sim_data"
    db_dir = "/Users/andradenebula/Documents/Research/Transient_Metrics/Multi_Transient_Metrics_Hub"

elif Shar:
    print("[CONFIG] Using Shar's Dirac server setup")
    sys.path.insert(0, "/lustre/lrspec/metrics")
    sys.path.insert(0, "/home/3155/metrics/Multi_Transient_Metrics_Hub")
    os.environ["RUBIN_SIM_DATA_DIR"] = "/lustre/lrspec/metrics/rubin_sim_data"
    db_dir = "/lustre/lrspec/metrics"

# Shared config
sys.path.append(os.path.abspath(".."))  # For shared_utils



#this happens twice because idk why but it only works like that
#...you still have to run it twice if it gives warnings
# C: Make sure you save your metric file before running. 

metric_filename = "new_names_local_GRBafterglows_metric"
s_u = "shared_utils"

# --- Reload metric module ---
if metric_filename in sys.modules:
    del sys.modules[metric_filename]
metric = __import__(metric_filename)
importlib.reload(metric)

# --- Reload shared_utils module ---
if s_u in sys.modules:
    del sys.modules[s_u]
shared_utils = __import__(s_u)
importlib.reload(shared_utils)

print(f"[INFO] Loaded metric module: {metric_filename}")
print(f"[INFO] Loaded shared_utils module")





#metric configurations

#control whether we generate new files
generate_new_templates = True
generate_new_pop = True
make_debug_plots = False #toggle whether or not the pop generation makes plots

#population variables
rate_density = 1e-8
dmin, dmax = 10, 1000
gal_lat_cut = None #latitude cut, for Galactic phenomena
t_start = 1 #start time in days
t_end = 3652

# Whether to remove Metric_temp_* folders after running
clean_temp = True  # <- NEW toggle

#cadence variables
cadences = ['four_roll_v4.3.1_10yrs', 'baseline_v4.3.1_10yrs']
ignore_triples = False #turn this to true to ignore triples
filters = ['u', 'g', 'r', 'i', 'z', 'y'] #doesn't work rn i think but
#if we wanted to look at less filters then we would adjust that here

# Standardized output paths for this science case
paths = metric.get_output_paths(case_label="GRBafterglows")  # <- can change to 'KNe' etc.

storage_dir = paths['storage_dir']
templates_file = paths['templates_file']
pop_file = paths['pop_file']



#load and/or generate light curves
shared_lc_model = metric.load_or_generate_templates(
    templates_file=templates_file,
    generate_new=generate_new_templates
)


#plot light curves from pkl file if desired
shared_utils.plot_some_lcs_from_pkl(templates_file, num=3)


# Load or generate population slicer
slicer = metric.load_or_generate_population(
    t_start=t_start,
    t_end=t_end,
    d_min=dmin,
    d_max=dmax,
    seed=42,
    num_lightcurves=1000,
    gal_lat_cut=gal_lat_cut,
    rate_density=rate_density,
    pop_file=pop_file,
    generate_new=generate_new_pop,
    make_debug_plots=make_debug_plots
)






#run detection metric
df_obs_arr = shared_utils.run_detect(metric, slicer, cadences, shared_lc_model, db_dir, storage_dir, debug=True, plot=True, clean_temp=clean_temp)


#choose what to run in run_multi_metrics
multi_metrics = metric.get_multi_metrics(shared_lc_model, include=['detect', 'characterize', 'spec_trigger', 'color_evolve', 'historical'])



shared_utils.run_multi_metrics(multi_metrics, slicer, cadences, shared_lc_model, db_dir, storage_dir, ignore_triples=False, plot=True, clean_temp=clean_temp)





#Note the new location of the ObsRecord dataframes!!
new_df = pd.read_csv("GRBafterglows_fixedpop_detectonly/rd1em08/Metric_temp_four_roll_v4.3.1_10yrs/ObsRecords_four_roll_v4.3.1_10yrs.csv", converters={'filter': eval})



#generally these transients are meant to fade in a few hours

plt.hist(df_obs_arr['fade_time_days'],bins=20)
plt.title("histogram of length of time that transient is detected (days)")
plt.xlabel("fade time (days)")


def plot_observed_light_curve(row, log=True, snr=3):
    '''
    takes a row from an obs dataframe, for instance: detected_df.iloc[4]
    returns nothing
    plots the observed light curve with each filter
    as a random color
    and events with snr>3 are bigger (open circles)
    nondetections are small dots
    '''
    unique_filters = np.unique(row['filter'])
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_filters)))
    filter_to_color = dict(zip(unique_filters, colors))
    color_array = np.array([filter_to_color[f] for f in row['filter']])


    snr_obs_mask = row['snr_obs'] > snr  # change threshold here
    snr_to_marker = {False: '.', True: 'o'}

    # plt.figure(figsize=(10, 6))

    # Loop over each filter and SNR condition
    for snr_cond in [True, False]:
            mask = (snr_obs_mask == snr_cond)
            if np.any(mask):  # Only plot if there are matching points
                if log:
                    plt.scatter(
                        np.log10(row['mjd_obs'][mask]),
                        row['mag_obs'][mask],
                        c=color_array[mask],
                        marker=snr_to_marker[snr_cond]
                    )
                else:
                    plt.scatter(
                        row['mjd_obs'][mask],
                        row['mag_obs'][mask],
                        c=color_array[mask],
                        marker=snr_to_marker[snr_cond]
                    )

    if log:
        plt.xlim(np.log10(row['peak_mjd'] - 10), np.log10(row['peak_mjd'] + 100))
    else:
        plt.xlim(row['peak_mjd'] - 10, row['peak_mjd'] + 100)
    plt.gca().invert_yaxis()
    if log:
        plt.xlabel("log MJD")
    else:
        plt.xlabel("MJD")
    plt.ylabel("Apparent mag")
    plt.title("Light Curve as Observed")
    plt.legend(["Large means detected",
                "Each color is a filter"])
    plt.tight_layout()
    plt.ylim(40,16)
    # plt.xscale("log")
    plt.show()



detected_df = df_obs_arr[df_obs_arr['detected']==True]
print(len(detected_df))


num_plotted=0
for i, row in detected_df.iterrows():
    plot_observed_light_curve(row,log=True)
    if num_plotted>3:
        break
    num_plotted+=1




rates = [estimate_fade_rate(shared_lc_model, i, "r", t1=1.0, t2=10.0) for i in range(len(shared_lc_model.data))]
plt.hist(rates, bins=30)
plt.xlabel("Fade Rate (mag/day)")
plt.ylabel("Number of Light Curves")
plt.title("Empirical Fade Rates from Synthetic GRB Light Curves")
plt.grid(True)
plt.show()


def compute_absolute_fade(lc_model, filtername="r", t1=1.0, t2=100.0):
    """
    Compute Δmag = m(t2) - m(t1) in a given filter for all GRB light curves.
    """
    deltas = []
    for i in range(len(lc_model.data)):
        mag_t1 = np.interp(t1, lc_model.data[i][filtername]['ph'],
                                 lc_model.data[i][filtername]['mag'])
        mag_t2 = np.interp(t2, lc_model.data[i][filtername]['ph'],
                                 lc_model.data[i][filtername]['mag'])
        deltas.append(mag_t2 - mag_t1)
    return np.array(deltas)

delta_mags = compute_absolute_fade(shared_lc_model, filtername='r')

plt.figure(figsize=(8, 5))
plt.hist(delta_mags, bins=30, color='steelblue', edgecolor='black')
plt.xlabel("Magnitude Increase from Day 1 to 100 (Δmag)")
plt.ylabel("Number of Light Curves")
plt.title("Absolute Magnitude Fading Over 100 Days (r-band)")
plt.grid(True)
plt.tight_layout()
plt.show()



# Load the CSV (after fixing how arrays were saved, as discussed)
# df = pd.read_csv("GRBafterglows_output/ObsRecords_four_roll_v4.3.1_10yrs.csv")
new_df = pd.read_csv("GRBafterglows_fixedpop_detectonly/rd1em08/Metric_temp_four_roll_v4.3.1_10yrs/ObsRecords_four_roll_v4.3.1_10yrs.csv", converters={'filter': eval})

# Count how many are detected vs not
detected_counts = new_df['detected'].value_counts()

print("Detection Summary:")
print(detected_counts)

# Optionally print percentages
print("\nDetection Percentages:")
print(detected_counts / len(df) * 100)



print(pd.DataFrame(obs_records).applymap(lambda x: len(x) if hasattr(x, '__len__') and not isinstance(x, str) else 'scalar').nunique()) #

